# Transformers-Sp25
Weekly seminar on transformers/LLMs at the University of Wyoming. Attached are
1) the lecture slides on the GPT architecture, scaling laws, and reasoning models (in-progress)
2) the demo notebook on BPE tokenization
3) the notes on miscellaneous other topics

At the beginning of this class, I heavily recommended [3Blue1Brown's playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) introducing the GPT architecture. Some of the other suggested readings:

## GPT architecture

- The original "Attention Is All You Need" [paper](https://dl.acm.org/doi/pdf/10.5555/3295222.3295349)
- Brandon Rohrer's [introduction](https://www.brandonrohrer.com/transformers)
- Andrej Karpathy's [playlist](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)

## Scaling

- Richard Sutton's ["The Bitter Lesson"](http://incompleteideas.net/IncIdeas/BitterLesson.html)
- Jacob Steinhardt's ["More is Different"](https://bounded-regret.ghost.io/more-is-different-for-ai/)

## Reasoning Models

- Noam Brown's [lecture](https://www.youtube.com/watch?v=eaAonE58sLU) on search
- Sasha Rush's [lecture](https://www.youtube.com/live/6fJjojpwv1I) on reward signals
- The DeepSeek-R1 [paper](https://arxiv.org/abs/2501.12948)

## CoT Monitoring

- OpenAI's [blog/paper](https://openai.com/index/chain-of-thought-monitoring/)
